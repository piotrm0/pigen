{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Memory\n",
    "# memory = Memory(location=\"./results_cache\", verbose=1)\n",
    "\n",
    "import shelve\n",
    "\n",
    "cache = shelve.open(\"cache2.pickle\", writeback=True)\n",
    "def memoize(func):\n",
    "    global cache\n",
    "    \n",
    "    fname = func.__name__\n",
    "    if fname not in cache:\n",
    "        cache[fname] = dict()\n",
    "\n",
    "    funccache = cache[fname]\n",
    "\n",
    "    def callfunc(*args):\n",
    "        global cache\n",
    "\n",
    "        if args in funccache:\n",
    "            return funccache[args]\n",
    "        else:\n",
    "            print(f\"executing {fname}\")\n",
    "            val = func(*args)\n",
    "            funccache[args] = val\n",
    "            cache[fname] = funccache\n",
    "            cache.sync()\n",
    "            \n",
    "            return val\n",
    "        \n",
    "    return callfunc\n",
    "\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fh = open(\"test.pickle\", \"wb\")\n",
    "fh.write(pickle.dumps(cache))\n",
    "fh.flush()\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open(\"test.pickle\", \"rb\")\n",
    "temp = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in temp.items():\n",
    "    cache[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275900994657640789512694683983525957098258226205224894077267194782684826014769909026401363944374553050682034962524517493996514314298091906592509372216964615157098583874105978859597729754989301617539284681382686838689427741559918559252459539594310499725246808459872736446958486538367362226260991246080512438843904512441365497627807977156914359977001296160894416948685558484063534220722258284886481584560285060168427394522674676788952521385225499546667278239864565961163548862305774564980355936345681743241125150760694794510965960940252288797108931456691368672287489405601015033086179286809208747609178249385890097149096759852613655497818931297848216829989487226588048575640142704775551323796414515237462343645428584447952658678210511413547357395231134271661021359695362314429524849371871101457654035902799344037420073105785390621983874478084784896833214457138687519435064302184531910484810053706146806749192781911979399520614196634287544406437451237181921799983910159195618146751426912397489409071864942319615679452080951465502252316038819301420937621378559566389377870830390697920773467221825625996615014215030680384477345492026054146659252014974428507325186660021324340881907104863317346496514539057962685610055081066587969981635747363840525714591028970641401109712062804390397595156771577004203378699360072305587631763594218731251471205329281918261861258673215791984148488291644706095752706957220917567116722910981690915280173506712748583222871835209353965725121083579151369882091444210067510334671103141267111369908658516398315019701651511685171437657618351556508849099898599823873455283316355076479185358932261854896321329330898570642046752590709154814165498594616371802709819943099244889575712828905923233260972997120844335732654893823911932597463667305836041428138830320382490375898524374417029132765618093773444030707469211201913020330380197621101100449293215160842444859637669838952286847831235526582131449576857262433441893039686426243410773226978028073189154411010446823252716201052652272111660396665573092547110557853763466820653109896526918620564769312570586356620185581007293606598764861179104533488503461136576867532494416680396265797877185560845529654126654085306143444318586769751456614068007002378776591344017127494704205622305389945613140711270004078547332699390814546646458807972708266830634328587856983052358089330657574067954571637752542021149557615814002501262285941302164715509792592309907965473761255176567513575178296664547791745011299614890304639947132962107340437518957359614589019389713111790429782856475032031986915140287080859904801094121472213179476477726224142548545403321571853061422881375850430633217518297986622371721591607716692547487389866549494501146540628433663937900397692656721463853067360965712091807638327166416274888800786925602902284721040317211860820419000422966171196377921337575114959501566049631862947265473642523081770367515906735023507283540567040386743513622224771589150495309844489333096340878076932599397805419341447377441842631298608099888687413260472156951623965864573021631598193195167353812974167729478672422924654366800980676928238280689964004824354037014163149658979409243237896907069779422362508221688957383798623001593776471651228935786015881617557829735233446042815126272037343146531977774160319906655418763979293344195215413418994854447345673831624993419131814809277771038638773431772075456545322077709212019051660962804909263601975988281613323166636528619326686336062735676303544776280350450777235547105859548702790814356240145171806246436267945612753181340783303362542327839449753824372058353114771199260638133467768796959703098339130771098704085913374641442822772634659470474587847787201927715280731767907707157213444730605700733492436931138350493163128404251219256517980694113528013147013047816437885185290928545201165839341965621349143415956258658655705526904965209858033850722426482939728584783163057777560688876446248246857926039535277348030480290058760758251047470916439613626760449256274204208320856611906254543372131535958450687724602901618766795240616342522577195429162991930645537799140373404328752628889639958794757291746426357455254079091451357111369410911939325191076020825202618798531887705842972591677813149699009019211697173727847684726860849003377024242916513005005168323364350389517029893\"\n",
    "print(len(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "typs = [\n",
    "    (\"float16\", np.float16), # np.half,\n",
    "    (\"float32\", np.float32), # np.single,\n",
    "    (\"float64\", np.float64), # np.double, # builtins.float\n",
    "    (\"float128\", np.float128), # np.longdouble\n",
    "]\n",
    "#for typ in typs:\n",
    "#    print(typ, np.finfo(typ[1]))\n",
    "\n",
    "zeroish = np.finfo(np.float64).tiny\n",
    "print(zeroish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/abacaj/awesome-transformers\n",
    "\n",
    "model_names = [\n",
    "    \"bigscience/bloom-560m\", \"bigscience/bloom-1b1\", \"bigscience/bloom-1b7\", \n",
    "    \"bigscience/bloom-3b\", \"bigscience/bloom-7b1\", \n",
    "    \"openai-gpt\", # gpt 1\n",
    "    \"distilgpt2\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\",\n",
    "    \"facebook/opt-125m\", \"facebook/opt-350m\", \"facebook/opt-1.3b\", \"facebook/opt-2.7b\", \n",
    "    \"facebook/opt-6.7b\",\n",
    "    \"EleutherAI/pythia-70m\", \n",
    "    \"EleutherAI/pythia-160m\", \n",
    "    \"EleutherAI/pythia-410m\", \n",
    "    \"EleutherAI/pythia-1b\", \n",
    "    \"EleutherAI/pythia-1.4b\", \n",
    "    \"EleutherAI/pythia-2.8b\", \n",
    "    \"EleutherAI/pythia-6.9b\", \n",
    "    \"EleutherAI/pythia-70m-deduped\", \n",
    "    \"EleutherAI/pythia-160m-deduped\", \n",
    "    \"EleutherAI/pythia-410m-deduped\", \n",
    "    \"EleutherAI/pythia-1b-deduped\", \n",
    "    \"EleutherAI/pythia-1.4b-deduped\", \n",
    "    \"EleutherAI/pythia-2.8b-deduped\", \n",
    "    \"EleutherAI/pythia-6.9b-deduped\", \n",
    "    \"xlnet-base-cased\"\n",
    "]\n",
    "\n",
    "model_names_big = [\n",
    "    \"EleutherAI/pythia-12b\", \n",
    "    \"EleutherAI/pythia-12b-deduped\",\n",
    "    \"facebook/opt-13b\", \n",
    "]\n",
    "\n",
    "model_names_too_big = [\n",
    "    # \"facebook/opt-30b\", # 'facebook/opt-66b'\n",
    "    # \"bigscience/bloom\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "pat = re.compile(r'[0-9]+')\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "floaty = np.float128\n",
    "\n",
    "def load_model(model_name, dtype=torch.float32):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    first_device = device\n",
    "\n",
    "    if \"openai\" not in model_name and \"xlnet\" not in model_name:\n",
    "        extras = dict(device_map=\"auto\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype, **extras)#.to(first_device)\n",
    "    else:\n",
    "        extras = dict()\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype, **extras).to(first_device)\n",
    "        \n",
    "    print(\"loading tokenizer\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(text):\n",
    "        return tok.batch_encode_plus(\n",
    "            [text],\n",
    "            return_tensors='pt',\n",
    "            max_length=4096,\n",
    "            # padding=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        ).to(first_device)\n",
    "\n",
    "    def probs_next(text, conditions=None):\n",
    "        temp = model.forward(**tokenize(text))\n",
    "        logits = temp['logits'][0,-1]\n",
    "    \n",
    "        probits = torch.nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "        if conditions != None:\n",
    "            \n",
    "            #if not isinstance(conditions, torch.Tensor):\n",
    "            #    conditions = torch.tensor(conditions).to(first_device)\n",
    "            # logits = torch.index_select(input=logits, dim=0, index=conditions)\n",
    "\n",
    "            cond_prob = floaty(probits[conditions].detach().cpu().numpy().sum())\n",
    "\n",
    "            return probits / cond_prob\n",
    "        else:\n",
    "            # pass\n",
    "            return probits\n",
    "\n",
    "    def prob_next(text, tok_id, conditions=None):\n",
    "        probits = probs_next(text, conditions)\n",
    "\n",
    "        tok_prob = floaty(probits[tok_id].detach().cpu().numpy().item())\n",
    "        return tok_prob\n",
    "    \n",
    "    return model, tok, tokenize, prob_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "# vocab = {k: tok.decode(k) for k in range(tok.vocab_size)}\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(cache['get_stats_for_model']):\n",
    "    print(k, len(k))\n",
    "    if len(k) < 3:\n",
    "        del cache['get_stats_for_model'][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in cache.items():\n",
    "    print(k)\n",
    "    for k2,v2 in v.items():\n",
    "        print(\"  \", k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize\n",
    "def get_stats_for_model(model_name, start_digit, dtype):\n",
    "    print(model_name)\n",
    "\n",
    "    model, tok, tokenize, prob_next = load_model(model_name, dtype)\n",
    "\n",
    "    vocab = {tok.decode(i): i for i in range(tok.vocab_size)}\n",
    "\n",
    "    num_toks = []\n",
    "\n",
    "    for k, i in vocab.items():\n",
    "        if pat.fullmatch(k):\n",
    "            num_toks.append((k, i))\n",
    "\n",
    "    print(f\"tokenizer has {len(num_toks)} digit(s) tokens\")\n",
    "\n",
    "    num_tok_ids = [nt[1] for nt in num_toks]\n",
    "\n",
    "    def next_toks(pos):\n",
    "        ret = []\n",
    "        for tok in num_toks:\n",
    "            if pi[pos:].startswith(tok[0]):\n",
    "                ret.append(tok)\n",
    "        return ret\n",
    "    \n",
    "    def runsteps(i):\n",
    "        state = defaultdict(floaty)\n",
    "        state[i] = floaty(1.0)\n",
    "        finals = defaultdict(floaty)\n",
    "\n",
    "        print(\"starting \", pi[0:i+1])\n",
    "\n",
    "        while len(state) > 0:\n",
    "            print(\"\\r\", state, end=\"\")\n",
    "            step(state, finals)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        return finals\n",
    "\n",
    "    def step(state: Dict, finals: Dict):\n",
    "        pos, prob = next(iter(state.items()))\n",
    "\n",
    "        del state[pos]\n",
    "\n",
    "        finals[pos] += prob            \n",
    "        if prob <= zeroish:\n",
    "            return\n",
    "\n",
    "        for step in next_toks(pos):\n",
    "            state[pos + len(step[0])] += prob * prob_next(pi[0:pos], step[1], num_tok_ids)\n",
    "\n",
    "    def n_or_more(probs, n):\n",
    "        if n >= len(probs):\n",
    "            return 0.0\n",
    "        \n",
    "        p = probs[n]\n",
    "\n",
    "        return p + (1.0 - p) * n_or_more(probs, n+1)\n",
    "    \n",
    "    q = runsteps(start_digit)\n",
    "\n",
    "    texts = [pi[i] for i in q.keys()]\n",
    "    probs = list(q.values())\n",
    "    cumul = [n_or_more(probs, i) for i in range(len(probs))]\n",
    "\n",
    "    texts = [pi[0:start_digit]] + texts\n",
    "    probs = [1.0] + probs\n",
    "    cumul = [1.0] + cumul\n",
    "\n",
    "    return q, texts, probs, cumul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress_probs_small = {model_name: get_stats_for_model(model_name, 3, torch.float32) for model_name in model_names}\n",
    "\n",
    "ress_probs_big = {model_name: get_stats_for_model(model_name, 3, torch.float16) for model_name in model_names_big}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ress = {k: v for k, v in ress_probs.items() if \"pythia\" in k}\n",
    "ress = dict()\n",
    "ress.update(ress_probs_small)\n",
    "ress.update(ress_probs_big)\n",
    "\n",
    "falloffs = dict()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams['text.antialiased'] = True\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11,5))\n",
    "\n",
    "mini = 0\n",
    "maxi = 1600\n",
    "\n",
    "colors = [f'C{i}' for i in range(1, 40)]\n",
    "\n",
    "ymin = 0.5**140\n",
    "\n",
    "xoff = 0.0 # -0.2\n",
    "\n",
    "ys = []\n",
    "\n",
    "w = 9\n",
    "indi = 0.1 ** w\n",
    "\n",
    "for model_name, (q, texts, probs, cumul) in ress.items():\n",
    "    color = colors[0]\n",
    "    colors = colors[1:]\n",
    "\n",
    "    xs = np.array(list(q.keys())).astype(float) - 1\n",
    "\n",
    "    xts = xs.copy()\n",
    "\n",
    "    xts[0] = xs[0] - 0.75\n",
    "    xts[1:] = xs[1:] + xoff\n",
    "\n",
    "    ploty = np.array(cumul)\n",
    "\n",
    "    emaxi = min(maxi, len(xs)) - w\n",
    "    emini = 0 # + (maxi - emaxi)\n",
    "\n",
    "    # print(len(xs), emini, emaxi)\n",
    "\n",
    "    ploty_ratio = (ploty[emini+w:emaxi+w] / ploty[emini:emaxi])# / 10.0\n",
    "                   #(np.log(cumul[mini+2:maxi+2]) - np.log(cumul[mini+1:maxi+1])) +\n",
    "                   #(np.log(cumul[mini+3:maxi+3]) - np.log(cumul[mini+2:maxi+2])) ) / 3.0\n",
    "\n",
    "    # ax.plot(xs[mini:maxi], cumul_ratio, color=color, alpha=0.1)\n",
    "\n",
    "    print(model_name, end=\" \")\n",
    "\n",
    "    indicated = np.argwhere(np.log(ploty_ratio) - log(indi) < 1)\n",
    "    if len(indicated) > 0:\n",
    "        fidx = indicated[0][0]\n",
    "        fy = ploty_ratio[fidx]\n",
    "\n",
    "        print(fidx, fy)\n",
    "\n",
    "        falloffs[model_name] = (fidx, fy)\n",
    "\n",
    "    else:\n",
    "        print(\"not forgotten, >\", len(ploty_ratio))\n",
    "\n",
    "        falloffs[model_name] = (len(ploty_ratio), None)\n",
    "\n",
    "    ax.scatter(xs[emini:emaxi], ploty_ratio, label=model_name, color=color, s=1.0, alpha=0.5)\n",
    "\n",
    "    ys.append(np.array(ploty_ratio))\n",
    "\n",
    "\n",
    "# ys = np.vstack(ys)\n",
    "\n",
    "#for x, y1, y2, t in list(zip(xts, ys.min(axis=0), ys.max(axis=0), texts))[mini:maxi]:\n",
    "#    if y2 > ymin:\n",
    "#        ax.annotate(t, (x, y2), xytext=(0.0, 1.0), textcoords=\"offset points\", horizontalalignment=\"center\", verticalalignment=\"bottom\", color=\"black\")#, family='monospace')\n",
    "#    if y1 > ymin and abs(log(y1)/log(0.5) - log(y2)/log(0.5)) > 5:\n",
    "#        ax.annotate(t, (x, y1), xytext=(0.0, -2.0), textcoords=\"offset points\", horizontalalignment=\"center\", verticalalignment=\"top\", color=\"black\")#, family='monospace')\n",
    "\n",
    "ax.annotate(text=None, xy=(0, indi), xytext=(maxi, indi), color=\"black\", arrowprops=dict(width=0.1, headwidth=0.0))\n",
    "# ax.annotate(\"\", (0, 1.0), (maxi, 1.0), color=\"black\", arrowprops=dict(width=0.1, headwidth=0.0))\n",
    "\n",
    "ax.set_yscale(\"log\", base=10)\n",
    "ax.set_ylim(0.1 ** (w+2), 10**2)\n",
    "ax.set_xlim(mini, maxi)\n",
    "\n",
    "ax.set_title(\"\"\"probability (y) of generating >= # (x) digits of π starting 3.14, conditional on generating digits\"\"\")\n",
    "# ax.legend(loc='upper left')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# yannot = ys.max(axis=0)\n",
    "\n",
    "#for yoff, typ in enumerate(typs):\n",
    "#    pin = str(typ[1](pi))\n",
    "#    n = len(pin)-1\n",
    "#    tipy = yannot[n-2]\n",
    "#    ax.annotate(\n",
    "#        typ[0],# + \" / \" + pin,\n",
    "#        xy=(n, tipy), xytext=(n, 0.5 ** (95-5*yoff)), xycoords='data', horizontalalignment=\"center\",\n",
    "#        arrowprops=dict(width=0.1, facecolor='black', shrink=0.00, headwidth=3.0, headlength=3.0, alpha=0.25)\n",
    "#    )\n",
    "\n",
    "# fig\n",
    "\n",
    "for m, (fx, fy) in falloffs.items():\n",
    "    if fy is None:\n",
    "        continue\n",
    "\n",
    "    ax.annotate(text=\"\", xy=(fx, fy), xytext=(fx, 1.0), color=\"black\", arrowprops=dict(width=0.5, headwidth=4.0, headlength=4.0, alpha=0.5), alpha=0.25)\n",
    "\n",
    "fig.savefig(\"falloff-pythia.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ress = {k: v for k, v in ress_probs.items() if \"pythia\" in k}\n",
    "ress = dict()\n",
    "ress.update(ress_probs_small)\n",
    "ress.update(ress_probs_big)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams['text.antialiased'] = True\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11,5))\n",
    "\n",
    "mini = 0\n",
    "maxi = 1600\n",
    "\n",
    "colors = [f'C{i}' for i in range(1, 40)]\n",
    "\n",
    "ymin = 0.5**140\n",
    "\n",
    "xoff = 0.0 # -0.2\n",
    "\n",
    "ys = []\n",
    "\n",
    "for model_name, (q, texts, probs, cumul) in ress.items():\n",
    "    color = colors[0]\n",
    "    colors = colors[1:]\n",
    "\n",
    "    xs = np.array(list(q.keys())).astype(float) - 1\n",
    "\n",
    "    xts = xs.copy()\n",
    "\n",
    "    xts[0] = xs[0] - 0.75\n",
    "    xts[1:] = xs[1:] + xoff\n",
    "\n",
    "    ploty = np.array(cumul)\n",
    "\n",
    "    emaxi = min(maxi, len(xs)) - w\n",
    "    emini = 0 # + (maxi - emaxi)\n",
    "\n",
    "    # print(len(xs), emini, emaxi)\n",
    "\n",
    "    ax.scatter(xs[emini:emaxi], ploty[emini:emaxi], label=model_name, color=color, s=0.5, alpha=0.5)\n",
    "\n",
    "    (fx, fy) = falloffs[model_name]\n",
    "    if fy is not None:\n",
    "        print(fx, fy)\n",
    "        fy = ploty[fx]\n",
    "        ax.annotate(text=\"\", xy=(fx, fy), xytext=(fx, 1.0), color=color, arrowprops=dict(color=color, width=0.5, headwidth=4.0, headlength=4.0, alpha=0.5), alpha=0.25)\n",
    "\n",
    "    ys.append(np.array(ploty))\n",
    "\n",
    "\n",
    "# ys = np.vstack(ys)\n",
    "\n",
    "#for x, y1, y2, t in list(zip(xts, ys.min(axis=0), ys.max(axis=0), texts))[mini:maxi]:\n",
    "#    if y2 > ymin:\n",
    "#        ax.annotate(t, (x, y2), xytext=(0.0, 1.0), textcoords=\"offset points\", horizontalalignment=\"center\", verticalalignment=\"bottom\", color=\"black\")#, family='monospace')\n",
    "#    if y1 > ymin and abs(log(y1)/log(0.5) - log(y2)/log(0.5)) > 5:\n",
    "#        ax.annotate(t, (x, y1), xytext=(0.0, -2.0), textcoords=\"offset points\", horizontalalignment=\"center\", verticalalignment=\"top\", color=\"black\")#, family='monospace')\n",
    "\n",
    "# ax.annotate(text=None, xy=(0, indi), xytext=(maxi, indi), color=\"black\", arrowprops=dict(width=0.1, headwidth=0.0))\n",
    "# ax.annotate(\"\", (0, 1.0), (maxi, 1.0), color=\"black\", arrowprops=dict(width=0.1, headwidth=0.0))\n",
    "\n",
    "ax.set_yscale(\"log\", base=10)\n",
    "# ax.set_ylim(0.1 ** (100), 10**2)\n",
    "ax.set_xlim(mini, maxi)\n",
    "\n",
    "ax.set_title(\"\"\"probability (y) of generating >= # (x) digits of π starting 3.14, conditional on generating digits\"\"\")\n",
    "# ax.legend(loc='upper left')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# yannot = ys.max(axis=0)\n",
    "\n",
    "#for yoff, typ in enumerate(typs):\n",
    "#    pin = str(typ[1](pi))\n",
    "#    n = len(pin)-1\n",
    "#    tipy = yannot[n-2]\n",
    "#    ax.annotate(\n",
    "#        typ[0],# + \" / \" + pin,\n",
    "#        xy=(n, tipy), xytext=(n, 0.5 ** (95-5*yoff)), xycoords='data', horizontalalignment=\"center\",\n",
    "#        arrowprops=dict(width=0.1, facecolor='black', shrink=0.00, headwidth=3.0, headlength=3.0, alpha=0.25)\n",
    "#    )\n",
    "\n",
    "# fig\n",
    "    \n",
    "fig.savefig(\"falloff-pythia.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy pi generation: starting 3.14, generate the most likely digit(s) until no longer pi\n",
    "\n",
    "@memoize\n",
    "def greedy_pi(model_name, start=\"3.14\"):\n",
    "    print(model_name)\n",
    "    \n",
    "    # model_name = model_names[0]\n",
    "    model, tok, tokenize, prob_next = load_model(model_name)\n",
    "\n",
    "    num_toks = []\n",
    "    vocab = {tok.decode(i): i for i in range(tok.vocab_size)}\n",
    "    for k, i in vocab.items():\n",
    "        if pat.fullmatch(k):\n",
    "            num_toks.append((k, i))\n",
    "    num_tok_ids = [nt[1] for nt in num_toks]\n",
    "\n",
    "    def greedy_next(text, conditions=None):\n",
    "        temp = model.forward(**tokenize(text))\n",
    "        logits = temp['logits'][0,-1]\n",
    "        probits = torch.nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "\n",
    "        if conditions is None:\n",
    "            mask = 1.0\n",
    "        else:\n",
    "            mask = np.zeros_like(probits)\n",
    "            np.put(mask, conditions, 1.0)\n",
    "\n",
    "        return np.argmax(probits*mask)\n",
    "\n",
    "    current = start\n",
    "\n",
    "    print(\"starting\", current)\n",
    "\n",
    "    while pi.startswith(current):\n",
    "        next_part = tok.decode(greedy_next(current,  num_tok_ids))\n",
    "        current += next_part\n",
    "\n",
    "    while not pi.startswith(current):\n",
    "        current = current[0:len(current)-1]\n",
    "\n",
    "    print(current, len(current))\n",
    "\n",
    "    return current, len(current)\n",
    "\n",
    "# greedy_pi(\"\", \"3.14159\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"3.14159\"\n",
    "\n",
    "ress_greedy_314159 = {model_name: greedy_pi(model_name, start) for model_name in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = dict()\n",
    "ress.update(ress_greedy_314159)\n",
    "#ress.update(ress_greedy_314159265)\n",
    "# ress.update(ress_greedy2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams['text.antialiased'] = True\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11,8))\n",
    "\n",
    "# cumul = [sum(probs[i:]) for i in range(len(probs))]\n",
    "\n",
    "mini = -0.5\n",
    "maxi = 100 #len(cumul)-1\n",
    "maxi = 2 ** 12.5\n",
    "\n",
    "# ax.set_yscale(\"log\", base=2)\n",
    "\n",
    "ax.set_xscale(\"symlog\", base=2)\n",
    "# ax.set_xticks(list(q.keys())[mini:maxi])\n",
    "# ax.set_xticklabels(texts[mini:maxi])#, rotation=90)\n",
    "\n",
    "#ax.set_xticks(list(range(0,10,2)) + list(range(10,200,5)), which=\"major\")\n",
    "#ax.set_xticks(list(range(0,200,1)), minor=True)\n",
    "# ax.set_xticklabels([], which=\"minor\")\n",
    "\n",
    "# ax.yaxis.set_major_formatter(lambda a,b: f\"1/2^{b}\" if b == 0 else str(b*10))\n",
    "# ax.set_yticks([0.5 ** i for i in range(0, 260, 10)])\n",
    "\n",
    "ax.grid(which='both', axis='x', visible=True, lw=0.1, c='gray')\n",
    "ax.set_xticks([2**i for i in range(1,12)])\n",
    "ax.set_yticks([], left=False)\n",
    "\n",
    "# ax.grid(which='major', axis='x', visible=True, lw=0.2, c='gray')\n",
    "# ax.grid(which='minor', axis='x', visible=True, lw=0.1, c='gray')\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "colors = [f'C{i}' for i in range(1, 40)]\n",
    "\n",
    "ymin = 0.5**140\n",
    "\n",
    "xoff = 0.0 # -0.2\n",
    "\n",
    "ys = []\n",
    "\n",
    "bar_y = []\n",
    "bar_width = []\n",
    "bar_color = []\n",
    "\n",
    "for y, (model_name, (maxpi, maxn)) in enumerate(ress.items()):\n",
    "    color = colors[0]\n",
    "    colors = colors[1:]\n",
    "\n",
    "    xs = np.array(list(range(maxn)))\n",
    "    # ys = np.array([y] * len(xs))\n",
    "    texts = np.array([pi[i] for i in xs])\n",
    "\n",
    "    # print(xs.shape, ys.shape, texts.shape)\n",
    "\n",
    "    ax.annotate(\n",
    "        xy=(xs[-1], y), xytext=(5.0, 0), \n",
    "        textcoords=\"offset points\", \n",
    "        text=f\"{model_name} / {len(start)-1}+{maxn-len(start)} digits\", color=color,\n",
    "        verticalalignment=\"center\",\n",
    "        )\n",
    "\n",
    "    for x, t in zip(xs, texts):\n",
    "        # print(x, y, t)\n",
    "        alpha = 1.0\n",
    "        # if x < len(start):\n",
    "            # alpha = 0.5\n",
    "        ax.annotate(xy=(x, y) , text=t, xytext=(-5,0), textcoords=\"offset points\", color=\"white\", alpha=alpha, horizontalalignment=\"right\", verticalalignment=\"center\")\n",
    "\n",
    "    bar_y.append(y)\n",
    "    bar_width.append(xs[-1]+1)\n",
    "    bar_color.append(color)\n",
    "\n",
    "ax.barh(y=bar_y, width=bar_width, color=bar_color, left=-1)\n",
    "\n",
    "for yoff, typ in enumerate(typs):\n",
    "    pin = str(typ[1](pi))\n",
    "    n = len(pin)-1\n",
    "    tipy = len(ress)\n",
    "    ax.annotate(\n",
    "        typ[0],\n",
    "        xy=(n, tipy), xytext=(n, -1.75 + 0.75*(yoff % 2)), xycoords='data', horizontalalignment=\"center\", backgroundcolor=\"white\",\n",
    "        arrowprops=dict(width=0.5, facecolor='black', shrink=0.00, headwidth=0.0, alpha=0.1)\n",
    "    )\n",
    "\n",
    "ax.set_title(f\"longest prefix, generating π greedily starting {start}, conditional on generating digits\")\n",
    "\n",
    "ax.set_xlim(mini, maxi)\n",
    "ax.set_ylim(-2.0, len(ress))\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"pigen-greedy.png\")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(**tokenize(\"hello\"))\n",
    "from ipywidgets import widgets, interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ress = []\n",
    "\n",
    "# @interact(input_text=widgets.Text(\"Fill in the name of these NBA players: Michael\", continuous_update=True))\n",
    "def generate(input_text):\n",
    "    global ress\n",
    "\n",
    "    if len(input_text) == 0: return\n",
    "\n",
    "    temp = model.forward(**tokenize(input_text))\n",
    "\n",
    "    logits = temp['logits'][0,-1].detach().cpu()\n",
    "    probits = torch.nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "    tops = torch.topk(probits, 10)\n",
    "\n",
    "    toks = tok.batch_decode(tops.indices)\n",
    "\n",
    "    ress = [str(toks)] + ress\n",
    "    ress = ress[0:10]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.bar(x=range(10), height=tops.values)\n",
    "    ax.set_xticks(range(len(toks)))\n",
    "    ax.set_xticklabels(toks)\n",
    "\n",
    "    for res in ress:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize\n",
    "def model_size(model_name):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"loading model {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)#, device_map=\"auto\")\n",
    "    \n",
    "    def product(its):\n",
    "        temp = 1\n",
    "        for it in its:\n",
    "            temp *= it\n",
    "        return temp\n",
    "\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        #print(p.dtype, product(p.shape))\n",
    "        total += product(p.shape)\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {m: model_size(m) for m in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for m in model_names:\n",
    "    size = sizes[m]\n",
    "    digits = ress_greedy_314159[m][1]\n",
    "\n",
    "    xs.append(size)\n",
    "    ys.append(digits-len(start))\n",
    "\n",
    "    dpp = 1024 * 1024 * 1024 * float(digits) / float(size)\n",
    "    # print(m, dpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_xscale(\"log\", base=2)\n",
    "ax.set_yscale(\"log\", base=2)\n",
    "ax.set_ylabel(\"[#] greedy π digits\")\n",
    "ax.set_xlabel(\"[#] model parameters\")\n",
    "ax.scatter(xs, ys, color=\"black\", s=10)\n",
    "ax.grid(visible=True, which='both')\n",
    "\n",
    "colors = [f'C{i}' for i in range(1, 40)]\n",
    "\n",
    "for i, (m, x, y) in enumerate(zip(model_names, xs, ys)):\n",
    "    ax.annotate(m, xy=(x,y), xytext=(3,3), textcoords=\"offset points\", color=colors[i])\n",
    "\n",
    "ax.set_title(f\"# of greedy π digits starting {start} vs. # of parameters\")\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"pigen-stats.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = f\"\"\"<table class=\"table\">\n",
    "<tr>\n",
    "<th>model</th>\n",
    "<th># parameters (G=2^30)</th>\n",
    "<th># of greedy π digits beyond the starting {start}</th>\n",
    "<th>digits per G params</th>\n",
    "</tr>\n",
    "\"\"\"\n",
    "for m, x, y in zip(model_names, xs, ys):\n",
    "    h += f\"\"\"<tr>\n",
    "      <td>{m}</td>\n",
    "      <td>{x/(2**30):0.2f} G</td>\n",
    "      <td>{y}</td>\n",
    "      <td>{(2**30)*y/x:0.1f}</td>\n",
    "    </tr>\"\"\"\n",
    "    \n",
    "h += \"\"\"\n",
    "</table>\n",
    "\"\"\"\n",
    "widgets.HTML(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_xscale(\"log\", base=2)\n",
    "ax.set_yscale(\"log\", base=2)\n",
    "ax.set_ylabel(\"[#] pre-amnesia π digits\")\n",
    "ax.set_xlabel(\"[#] model parameters\")\n",
    "\n",
    "ax.grid(visible=True, which='both')\n",
    "\n",
    "colors = [f'C{i}' for i in range(1, 40)]\n",
    "\n",
    "goodx = []\n",
    "goody = []\n",
    "\n",
    "for i, (m, x, y) in enumerate(zip(model_names, xs, ys)):\n",
    "    fx, fy = falloffs[m]\n",
    "    if fy is not None:\n",
    "        goodx.append(x)\n",
    "        goody.append(fx)\n",
    "\n",
    "ax.scatter(goodx, goody, color=\"black\", s=10)\n",
    "\n",
    "for i, (m, x, y) in enumerate(zip(model_names, xs, ys)):\n",
    "    fx, fy = falloffs[m]\n",
    "    if fy is not None:\n",
    "        ax.annotate(m, xy=(x,fx), xytext=(3,3), textcoords=\"offset points\", color=colors[i])\n",
    "\n",
    "ax.set_title(f\"# of π digits until amnesia vs. # of parameters\")\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"pigen-amnesia.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = f\"\"\"<table class=\"table\">\n",
    "<tr>\n",
    "<th>model</th>\n",
    "<th># parameters (G=2^30)</th>\n",
    "<th># of π digits before amnesia</th>\n",
    "<th>pre-amnesia digits per G params</th>\n",
    "</tr>\n",
    "\"\"\"\n",
    "for m, x, y in zip(model_names, xs, ys):\n",
    "    (fx, fy) = falloffs[m]\n",
    "\n",
    "    fx = float(fx)\n",
    "\n",
    "    if fy is None:\n",
    "        h += f\"\"\"<tr>\n",
    "          <td>{m}</td>\n",
    "          <td>{x/(2**30):0.2f} G</td>\n",
    "          <td>&gt; {int(fx)}</td>\n",
    "          <td>&gt; {(2**30)*fx/x:0.1f}</td>\n",
    "        </tr>\"\"\"\n",
    "    else:\n",
    "      h += f\"\"\"<tr>\n",
    "        <td>{m}</td>\n",
    "        <td>{x/(2**30):0.2f} G</td>\n",
    "        <td>{int(fx)}</td>\n",
    "        <td>{(2**30)*fx/x:0.1f}</td>\n",
    "      </tr>\"\"\"\n",
    "    \n",
    "h += \"\"\"\n",
    "</table>\n",
    "\"\"\"\n",
    "widgets.HTML(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from ipywidgets import widgets, interact\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe = pipe.to(\"cuda:1\")\n",
    "\n",
    "@interact(prompt=widgets.Text(continuous_update=False))\n",
    "def genimage(prompt):\n",
    "    image = pipe(prompt).images[0]\n",
    "    display(image)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d957b8e9181ae73f5937023d210398eeb5e64cb6fc2777fb99454b305fb3cc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
